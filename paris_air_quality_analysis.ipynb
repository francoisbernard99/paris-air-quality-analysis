{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Time Series Analysis: Paris PM2.5 & SO2\n",
    "\n",
    "**Portfolio Project 1: Air Quality Time Series Analysis**\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Data acquisition from EEA API\n",
    "- Data cleaning and validation\n",
    "- Time series analysis\n",
    "- Statistical analysis\n",
    "- Visualization with matplotlib\n",
    "\n",
    "**Data Source:** European Environment Agency (EEA)\n",
    "**Location:** Paris, France\n",
    "**Period:** January 2023\n",
    "**Pollutants:** PM2.5, SO2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "from scipy import stats\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_eea_data(country_code, city, pollutant, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Download air quality data from EEA API.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    country_code : str\n",
    "        Two-letter country code (e.g., 'FR')\n",
    "    city : str\n",
    "        City name (e.g., 'Paris')\n",
    "    pollutant : str\n",
    "        Pollutant code (e.g., 'PM2.5', 'SO2')\n",
    "    start_date : str\n",
    "        Start date in ISO format\n",
    "    end_date : str\n",
    "        End date in ISO format\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Path\n",
    "        Path to extracted data directory\n",
    "    \"\"\"\n",
    "    api_url = \"https://eeadmz1-downloads-api-appservice.azurewebsites.net/ParquetFile\"\n",
    "    \n",
    "    request_body = {\n",
    "        \"countries\": [country_code],\n",
    "        \"cities\": [city],\n",
    "        \"pollutants\": [pollutant],\n",
    "        \"dataset\": 1,\n",
    "        \"dateTimeStart\": start_date,\n",
    "        \"dateTimeEnd\": end_date,\n",
    "        \"aggregationType\": \"hour\"\n",
    "    }\n",
    "    \n",
    "    print(f\"Downloading {pollutant} data for {city}...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(api_url, json=request_body, timeout=300)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save and extract ZIP file\n",
    "        data_dir = Path('data/raw')\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        zip_path = data_dir / f\"{city.lower()}_{pollutant.lower()}_data.zip\"\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        extract_path = data_dir / f\"{city.lower()}_{pollutant.lower()}\"\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        \n",
    "        print(f\"✓ Downloaded and extracted to {extract_path}\")\n",
    "        return extract_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SO2 data for Paris (January 2023)\n",
    "paris_so2_path = download_eea_data(\n",
    "    country_code='FR',\n",
    "    city='Paris',\n",
    "    pollutant='SO2',\n",
    "    start_date='2023-01-01T00:00:00.000Z',\n",
    "    end_date='2023-01-31T23:59:59.000Z'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(data_path):\n",
    "    \"\"\"\n",
    "    Load and clean air quality data from parquet files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : Path\n",
    "        Path to directory containing parquet files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Cleaned dataframe\n",
    "    \"\"\"\n",
    "    # Find all parquet files\n",
    "    parquet_files = list(Path(data_path).glob('**/*.parquet'))\n",
    "    print(f\"Found {len(parquet_files)} parquet file(s)\")\n",
    "    \n",
    "    # Load all files\n",
    "    dfs = []\n",
    "    for file in parquet_files:\n",
    "        df = pd.read_parquet(file)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Loaded {len(df):,} raw measurements\")\n",
    "    \n",
    "    # Clean data\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Convert to numeric and handle invalid values\n",
    "    df['Value'] = pd.to_numeric(df['Value'], errors='coerce')\n",
    "    df = df[df['Value'] > -999]  # Remove invalid measurements\n",
    "    df = df.dropna(subset=['Value'])\n",
    "    \n",
    "    # Remove outliers (3 standard deviations)\n",
    "    z_scores = np.abs((df['Value'] - df['Value'].mean()) / df['Value'].std())\n",
    "    df = df[z_scores < 3]\n",
    "    \n",
    "    # Process datetime\n",
    "    df['DateTime'] = pd.to_datetime(df['Start'])\n",
    "    df['Date'] = df['DateTime'].dt.date\n",
    "    df['Hour'] = df['DateTime'].dt.hour\n",
    "    df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "    \n",
    "    # Sort by datetime\n",
    "    df = df.sort_values('DateTime').reset_index(drop=True)\n",
    "    \n",
    "    removed = original_count - len(df)\n",
    "    print(f\"Cleaned: {len(df):,} measurements ({removed} removed)\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean data\n",
    "if paris_so2_path:\n",
    "    df = load_and_clean_data(paris_so2_path)\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nData preview:\")\n",
    "    display(df[['DateTime', 'Value', 'Unit', 'Hour', 'DayOfWeek']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "stats_summary = {\n",
    "    'Mean': df['Value'].mean(),\n",
    "    'Median': df['Value'].median(),\n",
    "    'Std Dev': df['Value'].std(),\n",
    "    'Min': df['Value'].min(),\n",
    "    'Max': df['Value'].max(),\n",
    "    '25th Percentile': df['Value'].quantile(0.25),\n",
    "    '75th Percentile': df['Value'].quantile(0.75),\n",
    "    '95th Percentile': df['Value'].quantile(0.95)\n",
    "}\n",
    "\n",
    "print(\"\\n=== SO2 Concentration Statistics (µg/m³) ===\")\n",
    "for key, value in stats_summary.items():\n",
    "    print(f\"{key:20s}: {value:.2f}\")\n",
    "\n",
    "# WHO guideline (24-hour mean: 40 µg/m³)\n",
    "who_guideline = 40\n",
    "exceedances = (df['Value'] > who_guideline).sum()\n",
    "exceedance_pct = (exceedances / len(df)) * 100\n",
    "print(f\"\\nWHO Guideline Exceedances: {exceedances} ({exceedance_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 4 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Paris SO2 Air Quality Analysis - January 2023', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Time series\n",
    "axes[0, 0].plot(df['DateTime'], df['Value'], linewidth=0.8, alpha=0.7, color='steelblue')\n",
    "axes[0, 0].axhline(y=df['Value'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"Value\"].mean():.2f}')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('SO2 Concentration (µg/m³)')\n",
    "axes[0, 0].set_title('Time Series')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribution\n",
    "axes[0, 1].hist(df['Value'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].axvline(df['Value'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[\"Value\"].mean():.2f}')\n",
    "axes[0, 1].axvline(df['Value'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df[\"Value\"].median():.2f}')\n",
    "axes[0, 1].set_xlabel('SO2 Concentration (µg/m³)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of SO2 Concentrations')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Hourly patterns\n",
    "hourly_avg = df.groupby('Hour')['Value'].agg(['mean', 'std']).reset_index()\n",
    "axes[1, 0].plot(hourly_avg['Hour'], hourly_avg['mean'], marker='o', linewidth=2, color='steelblue')\n",
    "axes[1, 0].fill_between(hourly_avg['Hour'], \n",
    "                        hourly_avg['mean'] - hourly_avg['std'],\n",
    "                        hourly_avg['mean'] + hourly_avg['std'],\n",
    "                        alpha=0.3, color='steelblue')\n",
    "axes[1, 0].set_xlabel('Hour of Day')\n",
    "axes[1, 0].set_ylabel('SO2 Concentration (µg/m³)')\n",
    "axes[1, 0].set_title('Daily Pattern (Mean ± Std Dev)')\n",
    "axes[1, 0].set_xticks(range(0, 24, 3))\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Weekly patterns\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "daily_avg = df.groupby('DayOfWeek')['Value'].agg(['mean', 'std']).reset_index()\n",
    "axes[1, 1].bar(daily_avg['DayOfWeek'], daily_avg['mean'], color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].errorbar(daily_avg['DayOfWeek'], daily_avg['mean'], yerr=daily_avg['std'], \n",
    "                    fmt='none', ecolor='black', capsize=5)\n",
    "axes[1, 1].set_xlabel('Day of Week')\n",
    "axes[1, 1].set_ylabel('SO2 Concentration (µg/m³)')\n",
    "axes[1, 1].set_title('Weekly Pattern (Mean ± Std Dev)')\n",
    "axes[1, 1].set_xticks(range(7))\n",
    "axes[1, 1].set_xticklabels(day_names)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find peak and minimum hours\n",
    "peak_hour = hourly_avg.loc[hourly_avg['mean'].idxmax(), 'Hour']\n",
    "min_hour = hourly_avg.loc[hourly_avg['mean'].idxmin(), 'Hour']\n",
    "peak_value = hourly_avg['mean'].max()\n",
    "min_value = hourly_avg['mean'].min()\n",
    "\n",
    "print(\"\\n=== KEY FINDINGS ===\")\n",
    "print(f\"\\n1. Data Coverage:\")\n",
    "print(f\"   - Total measurements: {len(df):,}\")\n",
    "print(f\"   - Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"   - Data completeness: {(len(df) / (31 * 24)):.1%}\")\n",
    "\n",
    "print(f\"\\n2. Daily Patterns:\")\n",
    "print(f\"   - Peak hour: {int(peak_hour):02d}:00 ({peak_value:.2f} µg/m³)\")\n",
    "print(f\"   - Minimum hour: {int(min_hour):02d}:00 ({min_value:.2f} µg/m³)\")\n",
    "print(f\"   - Daily variation: {(peak_value - min_value):.2f} µg/m³\")\n",
    "\n",
    "weekday_avg = df[df['DayOfWeek'] < 5]['Value'].mean()\n",
    "weekend_avg = df[df['DayOfWeek'] >= 5]['Value'].mean()\n",
    "print(f\"\\n3. Weekly Patterns:\")\n",
    "print(f\"   - Weekday average: {weekday_avg:.2f} µg/m³\")\n",
    "print(f\"   - Weekend average: {weekend_avg:.2f} µg/m³\")\n",
    "print(f\"   - Difference: {abs(weekday_avg - weekend_avg):.2f} µg/m³\")\n",
    "\n",
    "print(f\"\\n4. Air Quality Assessment:\")\n",
    "print(f\"   - Mean concentration: {df['Value'].mean():.2f} µg/m³\")\n",
    "print(f\"   - Below WHO guideline: {100 - exceedance_pct:.1f}% of time\")\n",
    "if df['Value'].mean() < who_guideline:\n",
    "    print(f\"   - Status: Generally good air quality\")\n",
    "else:\n",
    "    print(f\"   - Status: Attention needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "output_dir = Path('data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / 'paris_so2_cleaned.parquet'\n",
    "\n",
    "df.to_parquet(output_file, index=False)\n",
    "print(f\"\\nCleaned data saved to: {output_file}\")\n",
    "print(f\"File size: {output_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This analysis demonstrates:\n",
    "- ✓ Data acquisition from EEA API\n",
    "- ✓ Data cleaning and validation\n",
    "- ✓ Time series analysis (trends, patterns)\n",
    "- ✓ Statistical analysis (mean, percentiles, exceedances)\n",
    "- ✓ Professional visualizations\n",
    "\n",
    "**Skills demonstrated:** Python, pandas, data cleaning, time series analysis, matplotlib, API integration, statistical analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
